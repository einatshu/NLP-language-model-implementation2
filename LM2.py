from math import log
import nltk
from nltk.lm.preprocessing import padded_everygram_pipeline
from nltk.lm import Laplace
from collections import Counter
from nltk.util import ngrams
from nltk import word_tokenize, sent_tokenize
import re
# nltk.download('punkt')


class nltk_language_model:
    """The class implements a Language Model that learns a model from a given text using nltk library.
    """

    n = 3
    lm = None
    ngram_model_as_dic = None

    def __init__(self, n=3):
        self.n = n

    @staticmethod
    def split_text_to_ngrams(text, ngram):
        """split text string into a list of ngrams
            Args:
                text: The text to split.
                ngram: ngram size.

            Returns:
                list with all strings as ngram.
        """
        words = text.split()
        start_string = ' '
        grouped_words = [start_string.join(words[i: i + ngram]) for i in range(0, (len(words) - ngram+1))]
        return grouped_words

    def build_model(self, text):
        """populates a dictionary counting all ngrams in the specified text.
                   Args:
                       text (str): the text to construct the model from.
        """
        text = normalize_text(text)
        tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]
        tokens = nltk.word_tokenize(text)
        train_data, padded_sents = padded_everygram_pipeline(self.n, tokenized_text)
        self.lm = Laplace(self.n)
        self.lm.fit(train_data, padded_sents)

        n_ngrams = ngrams(tokens, self.n)
        model_ngrams = Counter(n_ngrams)
        self.ngram_model_as_dic = {}
        for key, value in model_ngrams.items():
            self.ngram_model_as_dic[" ".join(key)] = value

    def get_model(self):
        """Returns the model as a dictionary of the form {ngram:count}
         """
        return self.ngram_model_as_dic

    def evaluate(self, text):
        """Returns the log-likelihod of the specified text to be generated by the model.
           Laplace smoothing should be applied if necessary.
           Args:
               text (str): Text to evaluate.
           Returns:
               Float. The float should reflect the (log) probability.
        """
        prob = 0
        for i in range(1, min(self.n, len(text.split()) + 1)):
            ngram = self.split_text_to_ngrams(text, i)[0]
            current_ngram_index = len(ngram.split())-1
            prob += self.lm.logscore(ngram.split()[current_ngram_index], ngram.split()[0:current_ngram_index])
        if self.n < len(text.split()) + 1:
            ngrams = self.split_text_to_ngrams(text, self.n)
            text_len = len(text.split()) - 1
            for i in range(1, min(len(ngrams)+1, text_len)):
                ngram = ngrams[i - 1]
                ngram_index = len(ngram.split()) - 1
                prob += self.lm.logscore(ngram.split()[ngram_index], ngram.split()[0:ngram_index])
        return prob


class Spell_Checker:
    """The class implements a context sensitive spell checker. The corrections
        are done in the Noisy Channel framework, based on a language model and
        an error distribution model.
    """
    lm = None
    error_table = None
    vocabulary = None

    @staticmethod
    def get_diff_indexes(string_1, string_2):
        """

        :param string_1:
        :param string_2:
        :return: return a list of the different indexes between the  given strings
        """
        return [i for i in range(len(string_1)) if string_1[i] != string_2[i]]

    @staticmethod
    def get_vocabulary_from_lm(lm_as_dict):
        """

        :param lm_as_dict: the object returned from get_model - dictionary of the form {ngram:count}
        :return: the vocabulary set
        """
        vocabulary = set()
        for ngram in lm_as_dict.keys():
            vocabulary.update(ngram.split())
        return vocabulary

    def get_error_type(self, error, correct):
        """

        :param error: error word
        :param correct: correct word
        :return: the error type as identify string
        """
        if len(error) < len(correct):
            return 'deletion'
        if len(error) > len(correct):
            return 'insertion'
        if len(self.get_diff_indexes(error, correct)) > 1:
            return 'transposition'
        else:
            return 'substitution'

    @staticmethod
    def get_non_word_errors(vocabulary, text_tokens):
        """

        :param vocabulary: the model vocabulary set - identify the words in the lm
        :param text_tokens: words list to decide if there are in the lm or not
        :return: list with words that are not in the lm (not in vocabulary)
        """
        return list(filter(lambda x: x not in vocabulary, text_tokens))

    def filter_words_in_vocabulary(self, candidates):
        """

        :param candidates: all possible candidate with edit distance
        :return: list with candidates that are in the lm (in vocabulary)
        """
        return list(filter(lambda x: x in self.vocabulary, candidates))

    @staticmethod
    def get_edit_distance_1(word):
        """

        :param word: the word to generate edit distance from
        :return: set of strings that are one edit away from `word`.
        """
        letters = 'abcdefghijklmnopqrstuvwxyz'
        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
        deletes = [L + R[1:] for L, R in splits if R]
        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]
        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]
        inserts = [L + c + R for L, R in splits for c in letters]
        return set(deletes + transposes + replaces + inserts)

    def get_edit_distance_2(self, edit_distance_1_strings):
        "All edits that are two edits away from `word`."
        edit_distance_2 = set()
        for e1 in edit_distance_1_strings:
            for e2 in self.get_edit_distance_1(e1):
                edit_distance_2.add(e2)
        return edit_distance_2

    @staticmethod
    def get_argmax(dic):
        """

        :param dic: dictionary
        :return: the key with the maximum value from dic
        """
        best_key = ""
        best_value = -float('Inf')
        for key, value in dic.items():
            if value > best_value:
                best_value = value
                best_key = key
        return best_key

    def find_substitution_error_chars(self, error, correct):
        """

        :param error: error word
        :param correct: correct word
        :return: the chars that differ in error and the chars that differ in correct on substitution error type
        """
        diff_index = self.get_diff_indexes(error, correct)
        chars_diff_in_error = error[diff_index[0]]
        chars_diff_in_correct = correct[diff_index[0]]
        return chars_diff_in_error, chars_diff_in_correct

    def find_transposition_error_chars(self, error, correct):
        """

        :param error: error word
        :param correct: correct word
        :return: the chars that differ in error and the chars that differ in correct on transposition error type
        """
        diff_index = self.get_diff_indexes(error, correct)
        chars_diff_in_error = error[diff_index[0]:diff_index[1] + 1]
        chars_diff_in_correct = correct[diff_index[0]:diff_index[1] + 1]
        return chars_diff_in_error, chars_diff_in_correct

    def find_deletion_insertion_error_chars(self, error, correct):
        """

        :param error: error word
        :param correct: correct word
        :return: the char that differ in error and correct and the previous it on deletion or insertion error type
        """
        diff_index = self.get_diff_indexes(error, correct)
        if not diff_index:
            diff_index = [len(error)]
        char_diff = correct[diff_index[0]]
        if diff_index[0] == 0:
            char_previous_diff = '@'
        else:
            char_previous_diff = correct[diff_index[0] - 1]
        return char_diff, char_previous_diff

    def handle_deletion_insertion_error_type(self, error_type, error, correct, error_distribution):
        """
                helper function for learn_error_distribution - count the values in deletion or insertion errors
        :param error_type: error type as describe in learn_error_distribution func
        :param error: error word
        :param correct: correct word
        :param error_distribution: dictionary built in learn_error_distribution
        """
        char_diff, char_previous_diff = self.find_deletion_insertion_error_chars(error, correct)
        if (char_diff, char_previous_diff) in error_distribution[error_type]:
            error_distribution[error_type][(char_diff, char_previous_diff)] += 1
        else:
            error_distribution[error_type][(char_diff, char_previous_diff)] = 1

    def find_candidates_to_error(self, error):
        """
        :param error: error word
        :return: word in vocabulary that are in edit distance from error
        """
        edit_distance_1_options = self.get_edit_distance_1(error)
        # edit_distance_1_options.update(self.get_edit_distance_2(edit_distance_1_options))
        return self.filter_words_in_vocabulary(edit_distance_1_options)

    def get_edit_probability(self, candidate, error):
        """
        :param candidate: correct word
        :param error: error word
        :return: error distribution from error table
        """
        error_type = self.get_error_type(error, candidate)
        try:
            if error_type == 'deletion':
                char_diff, char_previous_diff = self.find_deletion_insertion_error_chars(error, candidate)
                return self.error_table[error_type][(char_diff, char_previous_diff)]
            if error_type == 'insertion':
                char_diff, char_previous_diff = self.find_deletion_insertion_error_chars(candidate, error)
                return self.error_table[error_type][(char_diff, char_previous_diff)]
            if error_type == 'transposition':
                chars_diff_in_error, chars_diff_in_correct = self.find_transposition_error_chars(error, candidate)
            else:
                if error_type == 'substitution':
                    chars_diff_in_error, chars_diff_in_correct = self.find_substitution_error_chars(error, candidate)
            return self.error_table[error_type][(chars_diff_in_error, chars_diff_in_correct)]
        except:
            # didn't found error distribution
            return 0

    def get_ngram_size_from_lm(self):
        """
        :return: extract the ngram - size which used to initial lm
        """
        return len(list(self.lm.get_model().keys())[0].split())

    def get_lm_probability(self, candidate, context):
        """
        :param candidate: candidate word to replace
        :param context: spell check text with candidate replaced with error
        :return: spell check lm evaluation on the relevant context
        """
        context_split = context.split()
        ngram_size = self.get_ngram_size_from_lm()
        if ngram_size == 1 or len(context_split) == 1:
            # prior evaluation
            return self.evaluate(candidate)
        if ngram_size > 1:
            candidate_index_in_context = context_split.index(candidate)
            # - ngram_size+1
            min_range_list = list(range(0, candidate_index_in_context-ngram_size+2))
            min_range_list.append(0)
            min_previus_index = max(min_range_list)
            max_range_list = list(range(candidate_index_in_context + ngram_size, len(context_split)))
            max_range_list.append(len(context_split))
            max_next_index = min(max_range_list)
            sentence_to_evaluate = " ".join(context_split[min_previus_index:max_next_index])
            return self.evaluate(sentence_to_evaluate)

    def get_best_candidate(self, error, candidates, context):
        """
        :param error: error word
        :param candidates: all candidates to replace error
        :param context: list of words spell check text
        :return: most likely candidate by noisy channel model
        """
        score = {}
        for candidate in candidates:
            context_copy = context.copy()
            channel = self.check_channel_prob(self.get_edit_probability(candidate, error))
            context_copy[context.index(error)] = candidate
            str_context = ' '.join(context_copy)
            lm_probability = self.get_lm_probability(candidate, str_context)
            score[candidate] = channel + lm_probability
        return self.get_argmax(score)

    def __init__(self, lm=None):
        """Initializing a spell checker object with a language model as an
        instance  variable.

        Args:
            lm: a language model object. Defaults to None
        """
        self.lm = lm
        self.error_table = None
        if lm:
            self.vocabulary = self.get_vocabulary_from_lm(lm.get_model())

    @staticmethod
    def update_chars_occurrences(ngram, correct, ngram_chars_distribution):
        """
        helper function for learn_error_distribution- calculate occurrences for chars distribution
        :param ngram: lm ngram size
        :param correct: correct word
        :param ngram_chars_distribution: dictionary that holds the values
        """
        correct = '@' + correct
        ngram_chars_list = ["".join(correct[i: i + ngram]) for i in range(0, (len(correct) - ngram + 1))]
        for two_ngram_chars in set(ngram_chars_list):
            if two_ngram_chars not in ngram_chars_distribution:
                ngram_chars_distribution[two_ngram_chars] = ngram_chars_list.count(two_ngram_chars)
            else:
                ngram_chars_distribution[two_ngram_chars] += ngram_chars_list.count(two_ngram_chars)

    def build_model(self, text, n=3):
        """Returns a language model object built on the specified text. The language
            model should support evaluate() the get_model() functions as defined
            in assignment #1.

            Args:
                text (str): the text to construct the model from.
                n (int): the order of the n-gram model (defaults to 3).

            Returns:
                A language model object
        """

        self.lm = nltk_language_model(n)
        self.lm.build_model(text)
        self.vocabulary = self.get_vocabulary_from_lm(self.lm.get_model())

    def add_language_model(self, lm):
        """Adds the specified language model as an instance variable.

            Args:
                lm: a language model object
        """
        self.lm = lm
        self.vocabulary = self.get_vocabulary_from_lm(lm.get_model())

    def learn_error_distribution(self, errors_file):
        """Returns a dictionary {str:dict} where str is in:
            <'deletion', 'insertion', 'transposition', 'substitution'> and the
            inner dict {tupple: float} represents the confution matrix of the
            specific errors, where tupple is (err, corr) and the float is the
            probability of such an error.
            Examples of such tupples are ('t', 's'), for deletion of a 't'
            after an 's', insertion of a 't' after an 's'  and substitution
            of 's' by a 't'; and example of a transposition tupple is ('ac','ca').
            In the case of insersion, the tuppe (i,j) reads as "i was mistakingly
            added after j". In the case of deletion, the tupple reads as
            "i was mistakingly ommitted after j"

            Notes:
                1. The error distributions could be represented in more efficient ways.
                    We ask you to keep it simple and straight forward for clarity.
                2. Ultimately, one can use only 'deletion' and 'insertion' and have
                    'substitution' and 'transposition' derived. Again,  we use all
                    four types explicitly in order to keep things simple.
            Args:
                errors_file (str): full path to the errors file. File format mathces
                            Wikipedia errors list.

            Returns:
                A dictionary of error distributions by error type (dict).
        """
        error_distribution = {'deletion': {}, 'insertion': {}, 'transposition': {}, 'substitution': {}}
        two_ngram_chars_occurrences = {}
        one_ngram_chars_occurrences = {}
        with open(errors_file) as fp:
            line = fp.readline()
            while line:
                error = line.split()[0]
                correct = line.split()[1]
                self.update_chars_occurrences(2, correct, two_ngram_chars_occurrences)
                self.update_chars_occurrences(1, correct, one_ngram_chars_occurrences)
                error_type = self.get_error_type(error, correct)
                if error_type == 'deletion':
                    self.handle_deletion_insertion_error_type(error_type, error, correct, error_distribution)
                else:
                    if error_type == 'insertion':
                        self.handle_deletion_insertion_error_type(error_type, correct, error, error_distribution)
                    else:
                        if error_type == 'transposition':
                            chars_diff_in_error, chars_diff_in_correct = self.find_transposition_error_chars(error, correct)
                        else:
                            # error_type == 'substitution'
                            chars_diff_in_error, chars_diff_in_correct = self.find_substitution_error_chars(error, correct)
                        if (chars_diff_in_error, chars_diff_in_correct) in error_distribution[error_type]:
                            error_distribution[error_type][(chars_diff_in_error, chars_diff_in_correct)] += 1
                        else:
                            error_distribution[error_type][(chars_diff_in_error, chars_diff_in_correct)] = 1
                line = fp.readline()
        for error_type, error_type_distribution in error_distribution.items():
            for key, value in error_type_distribution.items():
                if error_type == 'deletion':
                    key_as_str = key[1]+key[0]
                    error_type_distribution[key] = value / two_ngram_chars_occurrences[key_as_str]
                if error_type == 'insertion' or error_type == 'substitution':
                    error_type_distribution[key] = value / one_ngram_chars_occurrences[key[1]]
                if error_type == 'transposition':
                    error_type_distribution[key] = value / two_ngram_chars_occurrences[key[1]]

        self.error_table = error_distribution
        return error_distribution

    def add_error_tables(self, error_tables):
        """ Adds the speficied dictionary of error tables as an instance variable.

            Args:
                error_tables (dict): a dictionary of error tables in the format
                returned by  learn_error_distribution()
        """
        self.error_table = error_tables

    def evaluate(self, text):
        """Returns the log-likelihod of the specified text given the language
            model in use. Smoothing is applied on texts containing OOV words

           Args:
               text (str): Text to evaluate.

           Returns:
               Float. The float should reflect the (log) probability.
        """
        return self.lm.evaluate(text)

    def get_candidates_error_distribution(self, candidates, word, alpha):
        """
        :param candidates: all candidate to replace word
        :param word: word to replace
        :param alpha: the probability of keeping a lexical word as is.
        :return: dictionary with all candidates noisy channel probability (word with probability = alpha
        and all candidate distribute uniformly in (1-alpha) with there probability to the typo mistake from error table)
        """
        candidates_error_distribution = {}
        for candidate in candidates:
            if candidate != word:
                candidates_error_distribution[candidate] = self.get_real_word_channel_probability(candidate, word, alpha)
        sum_of_candidates_distribution = sum(candidates_error_distribution.values())
        for candidate in candidates:
            if candidate != word:
                candidates_error_distribution[candidate] = (candidates_error_distribution[candidate] / sum_of_candidates_distribution) * (1-alpha)
            else:
                candidates_error_distribution[candidate] = self.get_real_word_channel_probability(candidate, word,
                                                                                                  alpha)
        return candidates_error_distribution

    def spell_check(self, text, alpha):
        """ Returns the most probable fix for the specified text. Use a simple
            noisy channel model is the number of tokens in the specified text is
            smaller than the length (n) of the language model.

            Args:
                text (str): the text to spell check.
                alpha (float): the probability of keeping a lexical word as is.

            Return:
                A modified string (or a copy of the original if no corrections are made.)
        """
        text_lines = text.split('\n')
        text_lines = list(map(lambda x: normalize_text(x), text_lines))
        best_correction = ""
        line_index = 0
        for line in text_lines:
            text_split = line.split()
            non_word_errors = self.get_non_word_errors(self.vocabulary, text_split)
            if non_word_errors:
                # assume there's only one error
                error = non_word_errors[0]
                candidates = self.find_candidates_to_error(error)
                if not candidates:
                    best_correction += line
                best_candidate = self.get_best_candidate(error, candidates, text_split.copy())
                text_split[text_split.index(error)] = best_candidate
                best_correction += ' '.join(text_split)
                if line_index < len(text_lines)-1:
                    best_correction += '\n'
            else:
                # real-word errors
                real_word_corrections_score = {}
                for word in text_split:
                    candidates = self.find_candidates_to_error(word)
                    candidates_error_distribution = self.get_candidates_error_distribution(candidates, word, alpha)
                    for candidate in candidates:
                        text_split_copy = text_split.copy()
                        text_split_copy[text_split.index(word)] = candidate
                        real_word_correction = ' '.join(text_split_copy)
                        real_word_corrections_score[real_word_correction] = self.get_real_word_correction_score(candidate, text_split_copy,
                                                                                                                candidates_error_distribution)

                best_correction += self.get_argmax(real_word_corrections_score)
                if line_index < len(text_lines)-1:
                    best_correction += '\n'
            line_index += 1
        if best_correction == "":
            best_correction = text
        return best_correction

    def get_real_word_channel_probability(self, candidate, word, alpha):
        """
        :param candidate: candidate word
        :param word: word to replace
        :param alpha: the probability of keeping a lexical word as is.
        :return: returns the error table probability,  word with probability = alpha and all candidates
        with there probability to the typo mistake from error table +epsilon
        """
        if candidate == word:
            return alpha
        return self.get_edit_probability(candidate, word) + 0.0000000001

    def get_real_word_correction_score(self, candidate, context, candidates_error_distribution):
        """
        :param candidate: candidate word
        :param context: context as list of tokens
        :param candidates_error_distribution: dictionary with all candidates noisy channel probability
        :return: the logged probability for the candidate
        """
        channel = log(candidates_error_distribution[candidate])
        str_context = ' '.join(context)
        lm_probability = self.evaluate(str_context)
        return channel + lm_probability

    @staticmethod
    def check_channel_prob(channel_prob):
        """
        :param channel_prob: probability from error table
        :return: logged probability with epsilon
        """
        channel_log = channel_prob + 0.0000000001
        return log(channel_log)


def normalize_text(text):
    """
    :param text: text to evaluate/ create model from
    :return: normalized text
    """
    text = text.lower()
    pattern = r'''(?x)          # set flag to allow verbose regexps
        (?:[A-Z]\.)+          # abbreviations, e.g. U.S.A.
        | \w+(?:-\w+)*        # words with optional internal hyphens
        | \$?\d+(?:\.\d+)?%?  # currency and percentages, e.g. $12.40, 82%
        | \.\.\.              # ellipsis
        | [][.,;"'?():_`-]    # these are separate tokens; includes ], [
        '''
    regexp = re.compile(pattern)
    tokens = regexp.findall(text)
    return ' '.join(tokens)